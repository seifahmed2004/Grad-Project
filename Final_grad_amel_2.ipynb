{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9g46IUJFZJc",
        "outputId": "74b5d8a0-0ff7-401d-d3b6-4d296b870a33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  libportaudio2\n",
            "0 upgraded, 1 newly installed, 0 to remove and 1 not upgraded.\n",
            "Need to get 65.3 kB of archives.\n",
            "After this operation, 223 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libportaudio2 amd64 19.6.0-1.1 [65.3 kB]\n",
            "Fetched 65.3 kB in 1s (111 kB/s)\n",
            "Selecting previously unselected package libportaudio2:amd64.\n",
            "(Reading database ... 117528 files and directories currently installed.)\n",
            "Preparing to unpack .../libportaudio2_19.6.0-1.1_amd64.deb ...\n",
            "Unpacking libportaudio2:amd64 (19.6.0-1.1) ...\n",
            "Setting up libportaudio2:amd64 (19.6.0-1.1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.11) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install torchaudio --quiet\n",
        "!pip install sounddevice --quiet\n",
        "!pip install soundfile --quiet\n",
        "!apt-get install libportaudio2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torchaudio\n",
        "\n",
        "# Set the path where you want to download LibriSpeech\n",
        "DATA_DIR = \"/content/LibriSpeech\"\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "# Choose the subset you want to download\n",
        "# Options: \"train-clean-100\", \"train-clean-360\", \"train-other-500\", \"dev-clean\", \"test-clean\", etc.\n",
        "subset = \"train-clean-100\"\n",
        "\n",
        "# Download the dataset using torchaudio\n",
        "dataset = torchaudio.datasets.LIBRISPEECH(\n",
        "    root=DATA_DIR,\n",
        "    url=subset,\n",
        "    download=True\n",
        ")\n",
        "\n",
        "print(f\"LibriSpeech {subset} downloaded at {DATA_DIR}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTb0HQEyFb8Q",
        "outputId": "d7a1f313-713d-48b6-fe92-8beba2d51c4a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LibriSpeech train-clean-100 downloaded at /content/LibriSpeech\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# 1. IMPORTS\n",
        "# ==============================\n",
        "import os\n",
        "import numpy as np\n",
        "import librosa\n",
        "from scipy.io.wavfile import write\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Reshape, LSTM, Dense\n",
        "import sounddevice as sd\n",
        "from collections import Counter\n",
        "\n",
        "# ==============================\n",
        "# 2. PARAMETERS\n",
        "# ==============================\n",
        "SAMPLE_RATE = 16000\n",
        "N_MFCC = 13\n",
        "MAX_LEN = 100\n",
        "WINDOW_DURATION = 1.5  # seconds\n",
        "WINDOW_OVERLAP = 0.5   # seconds\n",
        "NUM_CLASSES = 50       # top 50 words\n",
        "\n",
        "# ==============================\n",
        "# 3. PATHS\n",
        "# ==============================\n",
        "LIBRI_PATH = \"/content/LibriSpeech/LibriSpeech/train-clean-100\"  # adjust to your downloaded subset\n",
        "\n",
        "# ==============================\n",
        "# 4. PARSE TRANSCRIPTS & COLLECT AUDIO FILES\n",
        "# ==============================\n",
        "file_paths = []\n",
        "sentences = []\n",
        "\n",
        "for root, dirs, files in os.walk(LIBRI_PATH):\n",
        "    for file in files:\n",
        "        if file.endswith(\".txt\"):\n",
        "            txt_path = os.path.join(root, file)\n",
        "            with open(txt_path, \"r\") as f:\n",
        "                for line in f:\n",
        "                    parts = line.strip().split(\" \", 1)\n",
        "                    if len(parts) < 2:\n",
        "                        continue\n",
        "                    file_id, sentence = parts\n",
        "                    wav_file = os.path.join(root, file_id + \".flac\")\n",
        "                    if os.path.exists(wav_file):\n",
        "                        file_paths.append(wav_file)\n",
        "                        sentences.append(sentence.lower())\n",
        "\n",
        "# ==============================\n",
        "# 5. TOP 50 WORDS\n",
        "# ==============================\n",
        "all_text = \" \".join(sentences).split()\n",
        "word_counts = Counter(all_text)\n",
        "CLASSES = [w for w,_ in word_counts.most_common(NUM_CLASSES)]\n",
        "\n",
        "def word_to_index(word):\n",
        "    return CLASSES.index(word) if word in CLASSES else None\n",
        "\n",
        "print(\"Top 50 words:\", CLASSES)\n",
        "\n",
        "# ==============================\n",
        "# 6. AUDIO PREPROCESSING\n",
        "# ==============================\n",
        "def preprocess_audio(file_path):\n",
        "    try:\n",
        "        audio, sr = librosa.load(file_path, sr=SAMPLE_RATE)\n",
        "        if np.max(np.abs(audio)) > 0:\n",
        "            audio = audio / np.max(np.abs(audio))\n",
        "        return audio, sr\n",
        "    except Exception as e:\n",
        "        print(f\"[SKIP] Error loading {file_path}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "def extract_mfcc(audio, sr):\n",
        "    mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=N_MFCC)\n",
        "    if mfcc.shape[1] < MAX_LEN:\n",
        "        pad_width = MAX_LEN - mfcc.shape[1]\n",
        "        mfcc = np.pad(mfcc, pad_width=((0,0),(0,pad_width)))\n",
        "    else:\n",
        "        mfcc = mfcc[:, :MAX_LEN]\n",
        "    return mfcc\n",
        "\n",
        "# ==============================\n",
        "# 7. CREATE TRAINING DATA\n",
        "# ==============================\n",
        "X, y = [], []\n",
        "\n",
        "for i, file_path in enumerate(file_paths):\n",
        "    sentence = sentences[i].split()\n",
        "    audio, sr = preprocess_audio(file_path)\n",
        "    if audio is None:\n",
        "        continue\n",
        "\n",
        "    window_samples = int(WINDOW_DURATION * sr)\n",
        "    step = int((WINDOW_DURATION - WINDOW_OVERLAP) * sr)\n",
        "    start_idx = 0\n",
        "\n",
        "    for word in sentence:\n",
        "        if word not in CLASSES:\n",
        "            continue\n",
        "        end_idx = start_idx + window_samples\n",
        "        if end_idx > len(audio):\n",
        "            break\n",
        "        chunk = audio[start_idx:end_idx]\n",
        "        mfcc = extract_mfcc(chunk, sr)\n",
        "        X.append(mfcc)\n",
        "        y.append(word_to_index(word))\n",
        "        start_idx += step\n",
        "\n",
        "X = np.array(X)[..., np.newaxis]\n",
        "y = to_categorical(y, NUM_CLASSES)\n",
        "print(\"Dataset shape:\", X.shape)\n",
        "\n",
        "# ==============================\n",
        "# 8. TRAIN/TEST SPLIT\n",
        "# ==============================\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ==============================\n",
        "# 9. CNN + LSTM MODEL\n",
        "# ==============================\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3,3), activation='relu', input_shape=(N_MFCC, MAX_LEN,1)),\n",
        "    MaxPooling2D((2,2)),\n",
        "    Conv2D(64, (3,3), activation='relu'),\n",
        "    MaxPooling2D((2,2)),\n",
        "    Reshape((-1, 64)),  # automatically infer time steps\n",
        "    LSTM(128),\n",
        "    Dense(NUM_CLASSES, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "# ==============================\n",
        "# 10. TRAIN MODEL\n",
        "# ==============================\n",
        "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# ==============================\n",
        "# 11. RECORD LIVE AUDIO\n",
        "# ==============================\n",
        "def record_live_audio(filename=\"live.wav\", duration=10, device=None):\n",
        "    try:\n",
        "        print(f\"Recording {duration} seconds...\")\n",
        "        audio = sd.rec(int(duration*SAMPLE_RATE), samplerate=SAMPLE_RATE, channels=1, dtype='float32', device=device)\n",
        "        sd.wait()\n",
        "        write(filename, SAMPLE_RATE, audio)\n",
        "        print(\"Recording finished:\", filename)\n",
        "        return filename\n",
        "    except Exception as e:\n",
        "        print(f\"[SKIP] Could not record live audio: {e}\")\n",
        "        return None\n",
        "\n",
        "# ==============================\n",
        "# 12. PREDICT CONTINUOUS SPEECH\n",
        "# ==============================\n",
        "def sliding_windows(audio, sr, window_duration=WINDOW_DURATION, overlap=WINDOW_OVERLAP):\n",
        "    window_samples = int(window_duration*sr)\n",
        "    step = int((window_duration-overlap)*sr)\n",
        "    chunks = []\n",
        "    for start in range(0, len(audio)-window_samples+1, step):\n",
        "        chunks.append(audio[start:start+window_samples])\n",
        "    return chunks\n",
        "\n",
        "def predict_continuous_speech(file_path):\n",
        "    audio, sr = preprocess_audio(file_path)\n",
        "    if audio is None:\n",
        "        return \"\"\n",
        "    chunks = sliding_windows(audio, sr)\n",
        "    results = []\n",
        "    for chunk in chunks:\n",
        "        mfcc = extract_mfcc(chunk, sr)\n",
        "        mfcc = mfcc[np.newaxis, ..., np.newaxis]\n",
        "        pred = model.predict(mfcc, verbose=0)\n",
        "        index = np.argmax(pred)\n",
        "        results.append(CLASSES[index])\n",
        "    return \" \".join(results)\n",
        "\n",
        "# ==============================\n",
        "# 13. RUN LIVE TEST\n",
        "# ==============================\n",
        "live_file = '/content/audiootesting.m4a'\n",
        "if live_file:\n",
        "    recognized_text = predict_continuous_speech(live_file)\n",
        "    print(\"Recognized Text:\", recognized_text)\n",
        "else:\n",
        "    print(\"Live audio skipped. You can use a pre-recorded file for testing.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Po0o0oYmFcjI",
        "outputId": "4cf63de5-68b3-41ca-ea34-63dc2ef09c25"
      },
      "execution_count": 8,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 50 words: ['the', 'and', 'of', 'to', 'a', 'in', 'i', 'was', 'he', 'that', 'it', 'his', 'had', 'as', 'with', 'you', 'for', 'her', 'but', 'is', 'not', 'she', 'at', 'on', 'be', 'him', 'they', 'by', 'have', 'this', 'my', 'were', 'which', 'all', 'from', 'so', 'said', 'one', 'me', 'we', 'there', 'their', 'no', 'when', 'an', 'or', 'them', 'would', 'if', 'who']\n",
            "Dataset shape: (322878, 13, 100, 1)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">98</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ reshape (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">98,816</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,450</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m98\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m320\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m49\u001b[0m, \u001b[38;5;34m32\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m47\u001b[0m, \u001b[38;5;34m64\u001b[0m)      │        \u001b[38;5;34m18,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m64\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ reshape (\u001b[38;5;33mReshape\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m98,816\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │         \u001b[38;5;34m6,450\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">124,082</span> (484.70 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m124,082\u001b[0m (484.70 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">124,082</span> (484.70 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m124,082\u001b[0m (484.70 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m8072/8072\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m432s\u001b[0m 53ms/step - accuracy: 0.1361 - loss: 3.5137 - val_accuracy: 0.1371 - val_loss: 3.5040\n",
            "Epoch 2/20\n",
            "\u001b[1m8072/8072\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m421s\u001b[0m 52ms/step - accuracy: 0.1355 - loss: 3.5057 - val_accuracy: 0.1371 - val_loss: 3.5035\n",
            "Epoch 3/20\n",
            "\u001b[1m8072/8072\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m411s\u001b[0m 51ms/step - accuracy: 0.1366 - loss: 3.5049 - val_accuracy: 0.1371 - val_loss: 3.5030\n",
            "Epoch 4/20\n",
            "\u001b[1m8072/8072\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m421s\u001b[0m 52ms/step - accuracy: 0.1351 - loss: 3.5048 - val_accuracy: 0.1371 - val_loss: 3.5027\n",
            "Epoch 5/20\n",
            "\u001b[1m8072/8072\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m416s\u001b[0m 51ms/step - accuracy: 0.1342 - loss: 3.5086 - val_accuracy: 0.1371 - val_loss: 3.5029\n",
            "Epoch 6/20\n",
            "\u001b[1m8072/8072\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m409s\u001b[0m 51ms/step - accuracy: 0.1365 - loss: 3.5044 - val_accuracy: 0.1371 - val_loss: 3.5029\n",
            "Epoch 7/20\n",
            "\u001b[1m8072/8072\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m408s\u001b[0m 50ms/step - accuracy: 0.1345 - loss: 3.5053 - val_accuracy: 0.1371 - val_loss: 3.5026\n",
            "Epoch 8/20\n",
            "\u001b[1m8072/8072\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m410s\u001b[0m 51ms/step - accuracy: 0.1354 - loss: 3.5068 - val_accuracy: 0.1371 - val_loss: 3.5020\n",
            "Epoch 9/20\n",
            "\u001b[1m8072/8072\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m406s\u001b[0m 50ms/step - accuracy: 0.1357 - loss: 3.5057 - val_accuracy: 0.1371 - val_loss: 3.5028\n",
            "Epoch 10/20\n",
            "\u001b[1m8072/8072\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m407s\u001b[0m 50ms/step - accuracy: 0.1343 - loss: 3.5089 - val_accuracy: 0.1371 - val_loss: 3.5028\n",
            "Epoch 11/20\n",
            "\u001b[1m8072/8072\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m407s\u001b[0m 50ms/step - accuracy: 0.1354 - loss: 3.5052 - val_accuracy: 0.1371 - val_loss: 3.5022\n",
            "Epoch 12/20\n",
            "\u001b[1m8072/8072\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m446s\u001b[0m 51ms/step - accuracy: 0.1359 - loss: 3.5030 - val_accuracy: 0.1371 - val_loss: 3.5026\n",
            "Epoch 13/20\n",
            "\u001b[1m8072/8072\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m425s\u001b[0m 53ms/step - accuracy: 0.1366 - loss: 3.5064 - val_accuracy: 0.1371 - val_loss: 3.5020\n",
            "Epoch 14/20\n",
            "\u001b[1m8072/8072\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m411s\u001b[0m 51ms/step - accuracy: 0.1349 - loss: 3.5037 - val_accuracy: 0.1371 - val_loss: 3.5022\n",
            "Epoch 15/20\n",
            "\u001b[1m8072/8072\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m445s\u001b[0m 51ms/step - accuracy: 0.1364 - loss: 3.5032 - val_accuracy: 0.1371 - val_loss: 3.5022\n",
            "Epoch 16/20\n",
            "\u001b[1m8072/8072\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m411s\u001b[0m 51ms/step - accuracy: 0.1350 - loss: 3.5057 - val_accuracy: 0.1371 - val_loss: 3.5018\n",
            "Epoch 17/20\n",
            "\u001b[1m8072/8072\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m420s\u001b[0m 52ms/step - accuracy: 0.1351 - loss: 3.5039 - val_accuracy: 0.1371 - val_loss: 3.5026\n",
            "Epoch 18/20\n",
            "\u001b[1m8072/8072\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m405s\u001b[0m 50ms/step - accuracy: 0.1358 - loss: 3.5061 - val_accuracy: 0.1371 - val_loss: 3.5020\n",
            "Epoch 19/20\n",
            "\u001b[1m8072/8072\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m440s\u001b[0m 50ms/step - accuracy: 0.1354 - loss: 3.5051 - val_accuracy: 0.1371 - val_loss: 3.5018\n",
            "Epoch 20/20\n",
            "\u001b[1m8072/8072\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m424s\u001b[0m 53ms/step - accuracy: 0.1358 - loss: 3.5031 - val_accuracy: 0.1371 - val_loss: 3.5020\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1697152667.py:68: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio, sr = librosa.load(file_path, sr=SAMPLE_RATE)\n",
            "/usr/local/lib/python3.12/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recognized Text: the the the\n"
          ]
        }
      ]
    }
  ]
}